# The directory containing the pdfs to convert
pdf_dir: /lus/eagle/projects/argonne_tpc/siebenschuh/aurora_gpt/benchmark_data/mini_block_256_times_8

# The directory to place the converted pdfs in
out_dir: /lus/eagle/projects/argonne_tpc/siebenschuh/aurora_gpt/benchmark_data/output/experiment_256_times_8_updated_pymupdf_w_zip

# The settings for the pdf parser
parser_settings:
  # The name of the parser to use
  name: pymupdf

# Input is .zip rather than .pdf
iszip: true

# Temporary directory
tmp_storage: '/dev/shm'

# Number of file chunks (1 for zip)
chunk_size: 1

# The compute settings for the workflow
compute_settings:
  # The name of the compute platform to use
  name: polaris_cpu
  # The number of compute nodes to use
  num_nodes: 1
  # No of cpus for each worker -> total # of workers (impacted by chunksize)
  cores_per_worker: 4
  # Make sure to update the path to your conda environment and HF cache
  worker_init: "module use /soft/modulefiles; module load conda/2024-04-29; conda activate pymupdf-wf"
  # The scheduler options to use when submitting jobs
  scheduler_options: "#PBS -l filesystems=home:eagle"
  # Make sure to change the account to the account you want to charge
  account: FoundEpidem
  # The HPC queue to submit to
  queue: debug
  # The amount of time to request for your job
  walltime: 00:25:00
  monitoring_settings:
    resource_monitoring_interval: 10
    logging_endpoint: 'sqlite:////lus/eagle/projects/argonne_tpc/ogokdemir/adaparse/pymupdf/pymupdfZip_cpu_perf_log.db'
    workflow_name: pymupdfZip_cpu_perf_log_monitoring
